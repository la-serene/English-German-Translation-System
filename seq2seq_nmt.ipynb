{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbmWAwEjlvuO"
      },
      "source": [
        "# Neural Machine Translation: a seq2seq implementation to translate English to German"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8pS3PvNlwGC"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install \"tensorflow-text>=2.11\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQ9Qd9FygRzY"
      },
      "outputs": [],
      "source": [
        "# import necessary libs\n",
        "import numpy as np\n",
        "import re\n",
        "import gdown\n",
        "import os\n",
        "import random\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as layers\n",
        "import tensorflow.keras.utils as utils\n",
        "import tensorflow_text as tf_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB9eOWcv6M9c"
      },
      "source": [
        "### Introduction\n",
        "This notebook is to implement the seq2seq model proposed by [Sutskever et al.,2014.](https://arxiv.org/pdf/1409.3215.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ys474B6anhNe"
      },
      "outputs": [],
      "source": [
        "# Let's define some constant variables\n",
        "max_vocab_size = 20000\n",
        "dropout = .5\n",
        "\n",
        "BUFFER_SIZE = 256\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "embedding_size = 256\n",
        "hidden_units = 128\n",
        "\n",
        "data_file = \"deu.txt\"\n",
        "data_dir = '/content/data/'\n",
        "os.makedirs(data_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX9fhXvbgkrA"
      },
      "source": [
        "# Data Preprocessing\n",
        "We will use the [Kaggle English to German](https://www.kaggle.com/datasets/kaushal2896/english-to-german) data as the dataset on the English-German language pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6hwjDWEgwYa",
        "outputId": "949d2bf5-dfa3-4c43-edb1-1d269484f723"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-17 15:02:14--  https://drive.google.com/uc?export=download&id=1jqne1TQIir7usU7iBCCveOzCmtVLJcLU\n",
            "Resolving drive.google.com (drive.google.com)... 142.251.2.138, 142.251.2.102, 142.251.2.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.251.2.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1jqne1TQIir7usU7iBCCveOzCmtVLJcLU&export=download [following]\n",
            "--2024-01-17 15:02:14--  https://drive.usercontent.google.com/download?id=1jqne1TQIir7usU7iBCCveOzCmtVLJcLU&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.2.132, 2607:f8b0:4023:c0d::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.251.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8381019 (8.0M) [application/octet-stream]\n",
            "Saving to: ‘en_de.zip’\n",
            "\n",
            "en_de.zip           100%[===================>]   7.99M  39.9MB/s    in 0.2s    \n",
            "\n",
            "2024-01-17 15:02:16 (39.9 MB/s) - ‘en_de.zip’ saved [8381019/8381019]\n",
            "\n",
            "Archive:  en_de.zip\n",
            "  inflating: data/_about.txt         \n",
            "  inflating: data/deu.txt            \n"
          ]
        }
      ],
      "source": [
        "# Download dataset\n",
        "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1jqne1TQIir7usU7iBCCveOzCmtVLJcLU' -O en_de.zip\n",
        "!unzip en_de.zip -d data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtrXPgMadK_U",
        "outputId": "d8bfc6aa-9906-4388-d844-c263e5a08450"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go.\tGeh.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8597805 (Roujin)\n",
            "\n",
            "Hi.\tHallo!\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #380701 (cburgmer)\n",
            "\n",
            "Hi.\tGrüß Gott!\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #659813 (Esperantostern)\n",
            "\n",
            "Run!\tLauf!\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #941078 (Fingerhut)\n",
            "\n",
            "Run.\tLauf!\tCC-BY 2.0 (France) Attribution: tatoeba.org #4008918 (JSakuragi) & #941078 (Fingerhut)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Take a look at the first lines\n",
        "with open(os.path.join(data_dir, data_file)) as f:\n",
        "    for n, line in enumerate(f):\n",
        "        print(line)\n",
        "\n",
        "        if n == 4:\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xvp2AJCdep6E"
      },
      "source": [
        "To maintain coherence and prevent loss of word's meaning, contracted terms are expanded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pf_Re-bOB1F"
      },
      "outputs": [],
      "source": [
        "contraction_mapping = {\n",
        "    # This should be wrapped as a JSON file.\n",
        "    \"Let's\": \"Let us\",\n",
        "    \"'d better\": \" had better\",\n",
        "    \"'s\": \" is\",\n",
        "    \"'re\": \" are\",\n",
        "    \"n't\": \" not\",\n",
        "    \"'m\": \" am\",\n",
        "    \"'ll\": \" will\",\n",
        "    \"'d\": \" would\",\n",
        "    \"'ve\": \" have\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"cannot\": \"can not\"\n",
        "}\n",
        "\n",
        "def expand_contraction(text, mapping=contraction_mapping):\n",
        "    for contraction, expanded in mapping.items():\n",
        "        text = text.replace(contraction, expanded)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "pJmLUQbMe9qY",
        "outputId": "baef310a-ecfb-431a-df3e-bee4a4d3366b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'He definitely did not do it. He must be forced to commit crime.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Let's test the function\n",
        "expand_contraction(\"He definitely didn't do it. He must be forced to commit crime.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3WaWXfEdgch"
      },
      "source": [
        "The dataset are still in unprocessed form. It is necessary to preprocess and store them in appropriate form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tkv4Xv5jde1e",
        "outputId": "6476ebb1-a1be-4f38-9aaa-dff5f2fe256c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go.  --> <SOS> Geh. <EOS>\n",
            "Hi.  --> <SOS> Hallo! <EOS>\n",
            "Hi.  --> <SOS> Grüß Gott! <EOS>\n",
            "Run! --> <SOS> Lauf! <EOS>\n",
            "Run. --> <SOS> Lauf! <EOS>\n"
          ]
        }
      ],
      "source": [
        "en_set = []\n",
        "de_set = []\n",
        "\n",
        "with open(os.path.join(data_dir, data_file)) as f:\n",
        "    for line in f:\n",
        "        en_de = line.split(\"CC-BY\")\n",
        "        if len(en_de) > 0:\n",
        "            sample = en_de[0]\n",
        "            sample = sample.strip().split('\\t')\n",
        "            en_set.append(expand_contraction(sample[0]))\n",
        "            de_set.append(\"<SOS> \" + sample[1] + \" <EOS>\")\n",
        "\n",
        "for i in range(5):\n",
        "    print(\"{:4} --> {:4}\".format(en_set[i], de_set[i]))\n",
        "\n",
        "en_set = np.array(en_set)\n",
        "de_set = np.array(de_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "291A_tN4i98r",
        "outputId": "74dd94d1-afcd-4005-818b-36016cf291a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "221533"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Let' examine each the total size\n",
        "len(de_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajkxn2QA9LiH"
      },
      "outputs": [],
      "source": [
        "train_val_en, test_en, train_val_de, test_de = train_test_split(en_set, de_set, test_size=25000, random_state=25)\n",
        "train_en, val_en, train_de, val_de = train_test_split(train_val_en, train_val_de, test_size=25000, random_state=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmlYxEcK9z8U"
      },
      "outputs": [],
      "source": [
        "train_raw = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((train_en, train_de))\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "val_raw = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((val_en, val_de))\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "test_raw = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((test_en, test_de))\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kupf_MYAPQ4k"
      },
      "outputs": [],
      "source": [
        "en_length = [len(sentence[0]) for sentence in en_set]\n",
        "de_length = [len(sentence[1]) for sentence in de_set]\n",
        "time_periods = range(1, len(en_length) + 1)\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 4))  # Adjust figsize as needed\n",
        "\n",
        "# Create the first bar plot\n",
        "axs[0].bar(time_periods, en_length, alpha=0.7, edgecolor='black')\n",
        "axs[0].set_title('Length of English sentences')\n",
        "axs[0].set_xlabel('ith')\n",
        "axs[0].set_ylabel('Length')\n",
        "\n",
        "# Create the second bar plot\n",
        "axs[1].bar(time_periods, de_length, alpha=0.7, edgecolor='black')\n",
        "axs[1].set_title('Length of German sentences')\n",
        "axs[1].set_xlabel('ith')\n",
        "axs[1].set_ylabel('Length')\n",
        "\n",
        "# Adjust spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the subplots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WPpwFvJTn2p"
      },
      "source": [
        "It can be inferred from the plots that despite some outliers, sentence length tends to remain stable along both dataset. Therefore, it is not necessary to implement bucketing by length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkOTh8qpW3O1"
      },
      "source": [
        "# Text preprocessing\n",
        "Computer obviously cannot handle raw text. Instead, they need to be converted into numerical form for further calculations. Besides, while both removing punctuation and lowercasing all words are common practice in NLP tasks, it is not really the case for Neural Machine Translation. Punctuation is important to mark the start or end of a sentence. Therefore, we may well\n",
        "necessarily tokenize them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mca-vHiH-VNv"
      },
      "outputs": [],
      "source": [
        "def tf_lower_and_split_punct(text):\n",
        "\n",
        "    # Split accented characters.\n",
        "    text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "    text = tf.strings.lower(text)\n",
        "\n",
        "    # Keep space, a to z, and select punctuation.\n",
        "    text = tf.strings.regex_replace(text, r'[^ a-z.?!\\\\,<>]', '')\n",
        "\n",
        "    # Add spaces around punctuation.\n",
        "    text = tf.strings.regex_replace(text, r'[.?!,]', r' \\0 ')\n",
        "\n",
        "    # Strip whitespace.\n",
        "    text = tf.strings.strip(text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10ZA4Ptzkk9l"
      },
      "outputs": [],
      "source": [
        "# Vectorizer initial\n",
        "en_vec = layers.TextVectorization(max_tokens=max_vocab_size,\n",
        "                                  standardize=tf_lower_and_split_punct)\n",
        "de_vec = layers.TextVectorization(max_tokens=max_vocab_size,\n",
        "                                  standardize=tf_lower_and_split_punct)\n",
        "\n",
        "en_vec.adapt(train_raw.map(lambda src, tar: src))\n",
        "de_vec.adapt(train_raw.map(lambda src, tar: tar))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTSyhZqlsqkW"
      },
      "outputs": [],
      "source": [
        "# Vocabulary\n",
        "en_voc = en_vec.get_vocabulary()\n",
        "de_voc = de_vec.get_vocabulary()\n",
        "\n",
        "text_from_ids = {}\n",
        "for i in range(len(de_voc)):\n",
        "    text_from_ids[i] = de_voc[i]\n",
        "\n",
        "def get_text_from_ids(ids):\n",
        "    result = []\n",
        "\n",
        "    for i in ids:\n",
        "        result.append(text_from_ids[i])\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtyXRgGj-zf9",
        "outputId": "ecde0f6f-6501-48ba-9bb3-fb1a2184ebe4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14838\n",
            "20000\n"
          ]
        }
      ],
      "source": [
        "# Assign vocab size of each vectorizer\n",
        "input_vocab_size = len(en_vec.get_vocabulary())\n",
        "output_vocab_size = len(de_vec.get_vocabulary())\n",
        "print(input_vocab_size)\n",
        "print(output_vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-bVjVfLqvpg"
      },
      "source": [
        "# Model implementation\n",
        "In seq2seq, we need a RNN block, which is also known as Encoder, to encode the input sequence to a fixed-length vector, then another RNN block called Decoder to decode it. Block generally consists of LSTM cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXz0Ab-m6vBQ"
      },
      "source": [
        "## Encoder\n",
        "Encoder can be defined multi-layered RNN network. For the sake of simplicity, I will implement it as an one-layer RNN network with 1 cell at each timestep.\n",
        "\n",
        "Each RNN cell receives a source word and previous hidden state as inputs.\n",
        "\n",
        "\\begin{align*}\n",
        "s_{i}=tanh(Ws_{i-1}+Ux_{i})\n",
        "\\end{align*}\n",
        "\n",
        "According to the formula, the $i^{th}$ hidden state $s_{i}$ is calculated from the $(i-1)^{th}$ hidden state and the $i^{th}$ input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTiIOnGU7vcP"
      },
      "outputs": [],
      "source": [
        "# Let's define the encoder\n",
        "class Encoder(layers.Layer):\n",
        "    def __init__(self,\n",
        "                 hidden_units):\n",
        "        \"\"\"\n",
        "            Encoder Block in seq2seq\n",
        "\n",
        "        :param hidden_units: dimensionality of the output\n",
        "        :param tokenizer: tokenizer of the source language\n",
        "        \"\"\"\n",
        "\n",
        "        super(Encoder, self).__init__()\n",
        "        self.encoder_block = layers.LSTM(units=hidden_units,\n",
        "                                         dropout=dropout,\n",
        "                                         return_state=True)\n",
        "\n",
        "    def call(self,\n",
        "             src,\n",
        "             **kwargs):\n",
        "        \"\"\"\n",
        "            Calculate vector representation.\n",
        "\n",
        "        :param src: [batch, timesteps]\n",
        "\n",
        "        :return:\n",
        "            encoder_hidden_state: [batch, hidden_state_dim]\n",
        "            state_h: [batch, hidden_state_dim]\n",
        "            state_c: [batch, hidden_state_dim]\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        encoder_outputs, state_h, state_c = self.encoder_block(inputs=src,\n",
        "                                                               **kwargs)\n",
        "        return encoder_outputs, state_h, state_c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEobvMrrRRCi"
      },
      "source": [
        "## Decoder\n",
        "Encoder and Decoder share the same structure as well as hidden units but the last dense layer at each state which holds for predicting the next word using a softmax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKvgw4H_r5oG"
      },
      "outputs": [],
      "source": [
        "# Let's define the decoder\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 hidden_units):\n",
        "        \"\"\"\n",
        "            Decoder Block in seq2seq\n",
        "\n",
        "        :param hidden_units: dimensionality of the output\n",
        "        \"\"\"\n",
        "\n",
        "        super(Decoder, self).__init__()\n",
        "        self.decoder_block = layers.LSTM(units=hidden_units,\n",
        "                                         dropout=dropout,\n",
        "                                         return_sequences=True,\n",
        "                                         return_state=True)\n",
        "\n",
        "    def call(self,\n",
        "             trg,\n",
        "             previous_state,\n",
        "             **kwargs):\n",
        "        \"\"\"\n",
        "            Inputs:\n",
        "\n",
        "        :param trg: [batch, timesteps]\n",
        "        :param previous_state: [batch, hidden_unit_dim]\n",
        "\n",
        "        :return:\n",
        "            prediction: [vocab_size, None]\n",
        "        \"\"\"\n",
        "        decoder_outputs, state_h, state_c = self.decoder_block(inputs=trg,\n",
        "                                                               initial_state=previous_state,\n",
        "                                                               **kwargs)\n",
        "\n",
        "        return decoder_outputs, state_h, state_c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO5K_g9PaZC9"
      },
      "source": [
        "## seq2seq\n",
        "Now we have got the Encoder and Decoder. Let's combine them into seq2seq model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "312iMCYVZZk0"
      },
      "outputs": [],
      "source": [
        "class NMT(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 input_vocab_size,\n",
        "                 output_vocab_size,\n",
        "                 embedding_size,\n",
        "                 hidden_units):\n",
        "        \"\"\"\n",
        "            Initialize an instance for Neural Machine Translation Task\n",
        "\n",
        "        :param input_vocab_size: number of unique word in source language\n",
        "        :param output_vocab_size: number of unique word in target language\n",
        "        :param embedding_size: dimensionality of embedding layer\n",
        "        :param hidden_units: dimensionality of the output\n",
        "        \"\"\"\n",
        "\n",
        "        super(NMT, self).__init__()\n",
        "        self.e_embedding = layers.Embedding(input_dim=input_vocab_size,\n",
        "                                            output_dim=embedding_size)\n",
        "        self.d_embedding = layers.Embedding(input_dim=output_vocab_size,\n",
        "                                            output_dim=embedding_size)\n",
        "        self.encoder = Encoder(hidden_units)\n",
        "        self.decoder = Decoder(hidden_units)\n",
        "        self.dense = layers.Dense(output_vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self,\n",
        "             src,\n",
        "             trg):\n",
        "        e_mask = tf.not_equal(src, 0)\n",
        "        d_mask = tf.not_equal(trg, 0)\n",
        "\n",
        "        embed_src = self.e_embedding(src)\n",
        "        embed_trg = self.d_embedding(trg)\n",
        "\n",
        "        encoder_outputs, state_h, state_c = self.encoder(src=embed_src,\n",
        "                                                         mask=e_mask,\n",
        "                                                         training=True)\n",
        "        decoder_outputs, state_h, state_c = self.decoder(trg=embed_trg,\n",
        "                                                         previous_state=[state_h, state_c],\n",
        "                                                         mask=d_mask,\n",
        "                                                         training=True)\n",
        "        prediction = self.dense(decoder_outputs)\n",
        "\n",
        "        return prediction, state_h, state_c\n",
        "\n",
        "    def train(self,\n",
        "              dataset,\n",
        "              loss_fn,\n",
        "              optimizer,\n",
        "              epochs=5,\n",
        "              val_set=None):\n",
        "        \"\"\"\n",
        "            Train the model.\n",
        "\n",
        "        :param model: generator model\n",
        "        :param dataset: training dataset\n",
        "        :param loss_fn: loss function\n",
        "        :param optimizer: optimizer\n",
        "        :param epochs: number of training epochs\n",
        "        :param val_set: validation set\n",
        "        \"\"\"\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            loss_sum = 0\n",
        "            for step, (context, target) in enumerate(tqdm(dataset)):\n",
        "                tokenized_context = en_vec(context)\n",
        "                tokenized_target = de_vec(target)\n",
        "\n",
        "                # Apply Teacher Forcing (TF)\n",
        "                TF_target = tf.map_fn(lambda x: x[1:], tokenized_target)\n",
        "                tokenized_target = tf.map_fn(lambda x: x[:-1], tokenized_target)\n",
        "\n",
        "                with tf.GradientTape() as tape:\n",
        "                    prediction, _, _ = self(tokenized_context, tokenized_target)\n",
        "                    loss = loss_fn(TF_target, prediction)\n",
        "                    loss_sum += loss\n",
        "\n",
        "                gradients = tape.gradient(loss, self.trainable_weights)\n",
        "                optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
        "\n",
        "            val_loss_sum = 0;\n",
        "            if val_set is not None:\n",
        "                for step, (context, target) in enumerate(tqdm(val_set)):\n",
        "                    tokenized_context = en_vec(context)\n",
        "                    tokenized_target = de_vec(target)\n",
        "\n",
        "                    TF_target = tf.map_fn(lambda x: x[1:], tokenized_target)\n",
        "                    tokenized_target = tf.map_fn(lambda x: x[:-1], tokenized_target)\n",
        "\n",
        "                    prediction, _, _ = self(tokenized_context, tokenized_target)\n",
        "                    loss = loss_fn(TF_target, prediction)\n",
        "                    val_loss_sum += loss\n",
        "\n",
        "            print(\"\\nEpoch: {:1d}, loss = {:4f}, val_loss = {:4f}\".format(epoch, loss_sum, val_loss_sum))\n",
        "\n",
        "    def predict(self,\n",
        "                inputs):\n",
        "        \"\"\"\n",
        "            Generate translation from input.\n",
        "        \"\"\"\n",
        "        translation = []\n",
        "\n",
        "        tokenized_input = en_vec(inputs)\n",
        "        embed_input = self.e_embedding(tokenized_input)\n",
        "        embed_input = tf.expand_dims(embed_input, axis=0)\n",
        "\n",
        "        next_word = \"<sos>\"\n",
        "        de_vocab = de_vec.get_vocabulary()\n",
        "\n",
        "        encoder_state, state_h, state_c = self.encoder(src=embed_input,\n",
        "                                                       training=False)\n",
        "\n",
        "        for i in range(30):\n",
        "            if next_word != \"<eos>\":\n",
        "                tokenized_word = de_vec(next_word)\n",
        "                embed_word = self.d_embedding(tokenized_word)\n",
        "                embed_word = tf.expand_dims(embed_word, axis=0)\n",
        "\n",
        "                decoder_outputs, state_h, state_c = self.decoder(trg=embed_word,\n",
        "                                                            previous_state=[state_h, state_c],\n",
        "                                                            training=False)\n",
        "\n",
        "                prediction = self.dense(decoder_outputs)\n",
        "\n",
        "                dist = prediction.numpy().squeeze()\n",
        "                idx = np.random.choice(range(len(de_vocab)), p=dist)\n",
        "\n",
        "                next_word = de_vocab[tf.squeeze(idx)]\n",
        "\n",
        "                if next_word == \"[UNK]\":\n",
        "                    continue\n",
        "\n",
        "                translation.append(next_word)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0LLF70WDt1a"
      },
      "outputs": [],
      "source": [
        "nmt = NMT(input_vocab_size,\n",
        "          output_vocab_size,\n",
        "          embedding_size,\n",
        "          hidden_units)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "metadata": {
        "id": "FBsAwVvw9WCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNXHjwd6az-n",
        "outputId": "5f51d616-70e7-46c8-fc57-c4324e2d4bf9"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2681/2681 [13:51<00:00,  3.22it/s]\n",
            "100%|██████████| 391/391 [01:46<00:00,  3.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0, loss = 4453.133789, val_loss = 712.141541\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2681/2681 [13:53<00:00,  3.22it/s]\n",
            "100%|██████████| 391/391 [01:46<00:00,  3.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1, loss = 4134.282227, val_loss = 688.767029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2681/2681 [14:21<00:00,  3.11it/s]\n",
            "100%|██████████| 391/391 [01:40<00:00,  3.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 2, loss = 3879.782715, val_loss = 669.163818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2681/2681 [13:35<00:00,  3.29it/s]\n",
            "100%|██████████| 391/391 [02:21<00:00,  2.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 3, loss = 3673.938721, val_loss = 655.645447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2681/2681 [13:28<00:00,  3.32it/s]\n",
            "100%|██████████| 391/391 [01:41<00:00,  3.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 4, loss = 3505.182129, val_loss = 644.817932\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "nmt.train(train_raw, loss_fn, optimizer, epochs=5, val_set=val_raw)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nmt.save_weights(\"model_v1\")"
      ],
      "metadata": {
        "id": "xDnu9H5vC-BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frwEclI9pjnY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "fd755613-1df6-4bc1-8894-e47e175723cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'er spricht mir uber dreiig familie . <eos>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "\n",
        "\" \".join(nmt.predict(\"He asks me about his family.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLeRiTasQB18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "a018ed56-c260-4bac-a731-810d78868dbb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'er ist mein sohn . <eos>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "\" \".join(nmt.predict(\"He is my brother.\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nmt1 = NMT(input_vocab_size,\n",
        "          output_vocab_size,\n",
        "          embedding_size,\n",
        "          hidden_units)\n",
        "\n",
        "nmt1.load_weights(\"model_v1\")\n",
        "\n",
        "def extractTranslation(eng):\n",
        "    result = nmt1.predict(eng)\n",
        "    return \" \".join(result[:-1])"
      ],
      "metadata": {
        "id": "FPnI2BsuCxxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extractTranslation(\"It is such a beautiful day!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "tV85g3UvxRVC",
        "outputId": "e533206d-9433-4480-e27b-3fd7c5f2f992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'es ist so schones verschoben ein schoner tag .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r-NXAiPQxUBG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}