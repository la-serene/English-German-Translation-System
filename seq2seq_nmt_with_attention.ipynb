{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbmWAwEjlvuO"
      },
      "source": [
        "# Neural Machine Translation: a seq2seq implementation to translate English to German"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8pS3PvNlwGC"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install \"tensorflow-text>=2.11\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQ9Qd9FygRzY"
      },
      "outputs": [],
      "source": [
        "# import necessary libs\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "from typing import Any, Tuple\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_text as tf_text\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.utils as utils\n",
        "from tensorflow.keras.layers import (Layer, Dense, LSTM, Embedding,\n",
        "                                     TextVectorization, Bidirectional, Add,\n",
        "                                     LayerNormalization, AdditiveAttention,\n",
        "                                     StringLookup, Masking)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB9eOWcv6M9c"
      },
      "source": [
        "# Introduction\n",
        "This notebook is to implement the seq2seq model with Attention proposed by [Sutskever et al., 2014](https://arxiv.org/abs/1409.3215) and [Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ys474B6anhNe"
      },
      "outputs": [],
      "source": [
        "# Let's define some constant variables\n",
        "max_vocab_size = 15000\n",
        "DROPOUT = 0.5\n",
        "\n",
        "BUFFER_SIZE = 1024\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "embedding_size = 32\n",
        "hidden_units = 64\n",
        "\n",
        "data_file = \"deu.txt\"\n",
        "data_dir = '/content/data/'\n",
        "os.makedirs(data_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX9fhXvbgkrA"
      },
      "source": [
        "# Data Preprocessing\n",
        "We will use the [English to German](https://www.manythings.org/anki/deu-eng.zip) dataset from Manythings.org."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6hwjDWEgwYa",
        "outputId": "34fe4c27-af3a-4be7-cc07-7df5f833afa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-13 15:12:14--  https://www.manythings.org/anki/deu-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10364105 (9.9M) [application/zip]\n",
            "Saving to: ‘deu-eng.zip’\n",
            "\n",
            "deu-eng.zip         100%[===================>]   9.88M  27.1MB/s    in 0.4s    \n",
            "\n",
            "2024-04-13 15:12:15 (27.1 MB/s) - ‘deu-eng.zip’ saved [10364105/10364105]\n",
            "\n",
            "Archive:  deu-eng.zip\n",
            "  inflating: data/deu.txt            \n",
            "  inflating: data/_about.txt         \n"
          ]
        }
      ],
      "source": [
        "# Download dataset\n",
        "!wget --no-check-certificate 'https://www.manythings.org/anki/deu-eng.zip' -O deu-eng.zip\n",
        "!unzip deu-eng.zip -d data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtrXPgMadK_U",
        "outputId": "cceb7d27-5722-47d0-e53f-60a2654d9772"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go.\tGeh.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8597805 (Roujin)\n",
            "Hi.\tHallo!\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #380701 (cburgmer)\n",
            "Hi.\tGrüß Gott!\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #659813 (Esperantostern)\n",
            "Run!\tLauf!\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #941078 (Fingerhut)\n",
            "Run.\tLauf!\tCC-BY 2.0 (France) Attribution: tatoeba.org #4008918 (JSakuragi) & #941078 (Fingerhut)\n"
          ]
        }
      ],
      "source": [
        "# Take a look at the first lines\n",
        "with open(os.path.join(data_dir, data_file)) as f:\n",
        "    for n, line in enumerate(f):\n",
        "        print(line.strip())\n",
        "\n",
        "        if n == 4:\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xvp2AJCdep6E"
      },
      "source": [
        "To maintain coherence and prevent loss of word's meaning, contracted terms are expanded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pf_Re-bOB1F"
      },
      "outputs": [],
      "source": [
        "contraction_map = {\n",
        "    # This should be wrapped as a JSON file.\n",
        "    \"Let's\": \"Let us\",\n",
        "    \"'d better\": \" had better\",\n",
        "    \"'s\": \" is\",\n",
        "    \"'re\": \" are\",\n",
        "    \"'m\": \" am\",\n",
        "    \"'ll\": \" will\",\n",
        "    \"'d\": \" would\",\n",
        "    \"'ve\": \" have\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"n't\": \" not\",\n",
        "    \"cannot\": \"can not\"\n",
        "}\n",
        "\n",
        "def expand_contractions(text, mapping=contraction_map):\n",
        "    for key, value in mapping.items():\n",
        "        text = tf.strings.regex_replace(text, key, value)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJmLUQbMe9qY",
        "outputId": "6c268e8a-1b63-4ebe-febd-94c601eafa2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=string, numpy=b'He definitely did not do it. He must have been forced to commit crime. He will not do it again.'>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Let's test the function\n",
        "expand_contractions(\"He definitely didn't do it. He must've been forced to commit crime. He won't do it again.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3WaWXfEdgch"
      },
      "source": [
        "The dataset are still in unprocessed form. It is necessary to preprocess and store them in appropriate form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tkv4Xv5jde1e",
        "outputId": "45b2ae50-fd10-4826-d76e-db3ac1395cb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go.  --> Geh.\n",
            "Hi.  --> Hallo!\n",
            "Hi.  --> Grüß Gott!\n",
            "Run! --> Lauf!\n",
            "Run. --> Lauf!\n"
          ]
        }
      ],
      "source": [
        "english = []\n",
        "german = []\n",
        "\n",
        "with open(os.path.join(data_dir, data_file)) as f:\n",
        "    for line in f:\n",
        "        line = line.split(\"CC-BY\")\n",
        "\n",
        "        if len(line) > 0:\n",
        "            sample = line[0]\n",
        "            sample = sample.strip().split('\\t')\n",
        "\n",
        "            english.append(sample[0])\n",
        "            german.append(sample[1])\n",
        "\n",
        "english = np.array(english)\n",
        "german = np.array(german)\n",
        "\n",
        "for i in range(5):\n",
        "    print(\"{:4} --> {:4}\".format(english[i], german[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1skosGT9QI9d"
      },
      "outputs": [],
      "source": [
        "# english = english[40000:]\n",
        "# german = german[40000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_cRzMov9cxp"
      },
      "outputs": [],
      "source": [
        "train_ratio = 0.8\n",
        "val_ratio = 0.1\n",
        "test_ratio = 0.1\n",
        "\n",
        "train_mask = np.random.uniform(size=(len(english),)) < train_ratio\n",
        "val_mask = np.logical_and(~train_mask, np.random.uniform(size=(len(english),)) < val_ratio)\n",
        "test_mask = ~(train_mask | val_mask)\n",
        "\n",
        "train_raw = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((english[train_mask], german[train_mask]))\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "val_raw = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((english[val_mask], german[val_mask]))\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "test_raw = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((english[test_mask], german[test_mask]))\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WPpwFvJTn2p"
      },
      "source": [
        "It can be inferred from the plots that despite some outliers, sentence length tends to remain stable along both dataset. Therefore, it is not necessary to implement bucketing by length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkOTh8qpW3O1"
      },
      "source": [
        "# Tokenization\n",
        "Computer obviously cannot handle raw text. Instead, they need to be converted into numerical form for further calculations. Besides, while both removing punctuation and lowercasing all words are common practice in NLP tasks, it is not really the case for Neural Machine Translation. Punctuation is important to mark the start or end of a sentence. Therefore, we may well\n",
        "necessarily tokenize them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mca-vHiH-VNv"
      },
      "outputs": [],
      "source": [
        "def tf_lower_and_split_punct(text):\n",
        "  # Split accented characters.\n",
        "  text = expand_contractions(text)\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "\n",
        "  # Keep space, a to z, and select punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "\n",
        "  # Add spaces around punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "\n",
        "  # Strip whitespace.\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tg7W3RGP4lg6"
      },
      "outputs": [],
      "source": [
        "def tf_split_punct(text):\n",
        "  # Split accented characters.\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "\n",
        "  # Keep space, a to z, and select punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "\n",
        "  # Add spaces around punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "\n",
        "  # Strip whitespace and add special tokens\n",
        "  text = tf.strings.strip(text)\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10ZA4Ptzkk9l"
      },
      "outputs": [],
      "source": [
        "# Vectorizer initial\n",
        "en_vec = TextVectorization(max_tokens=max_vocab_size,\n",
        "                           standardize=tf_lower_and_split_punct)\n",
        "ger_vec = TextVectorization(max_tokens=max_vocab_size,\n",
        "                            standardize=tf_split_punct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_pKFciMqKGQ"
      },
      "outputs": [],
      "source": [
        "en_vec.adapt(train_raw.map(lambda x, y: x))\n",
        "ger_vec.adapt(train_raw.map(lambda x, y: y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTSyhZqlsqkW"
      },
      "outputs": [],
      "source": [
        "# Vocabulary\n",
        "en_voc = en_vec.get_vocabulary()\n",
        "ger_voc = ger_vec.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IgfCUpcJzKA"
      },
      "outputs": [],
      "source": [
        "# Word to Idx for prediction\n",
        "word_to_idx = {}\n",
        "\n",
        "for i in range(len(ger_voc)):\n",
        "    word_to_idx[ger_voc[i]] = i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtyXRgGj-zf9",
        "outputId": "ec624fe6-5015-4ef1-ef39-9a82b3212f7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15000\n",
            "15000\n"
          ]
        }
      ],
      "source": [
        "# Assign vocab size of each vectorizer\n",
        "input_vocab_size = len(en_vec.get_vocabulary())\n",
        "output_vocab_size = len(ger_vec.get_vocabulary())\n",
        "print(input_vocab_size)\n",
        "print(output_vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkbZvM9wl568"
      },
      "source": [
        "# Data Preparation\n",
        "Structure the dataset to use the tf.keras.models.Model's fit() method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJMddsdymBbP"
      },
      "outputs": [],
      "source": [
        "def process_text(context, target):\n",
        "  context = en_vec(context)\n",
        "  target = ger_vec(target)\n",
        "  targ_in = target[:, :-1]\n",
        "  targ_out = target[:, 1:]\n",
        "  return (context, targ_in), targ_out\n",
        "\n",
        "train_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
        "val_ds = val_raw.map(process_text, tf.data.AUTOTUNE)\n",
        "test_ds = test_raw.map(process_text, tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-bVjVfLqvpg"
      },
      "source": [
        "# Model implementation\n",
        "In seq2seq, we need a RNN block, which is also known as Encoder, to encode the input sequence to a fixed-length vector, then another RNN block called Decoder to decode it. Block generally consists of LSTM cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXz0Ab-m6vBQ"
      },
      "source": [
        "## Encoder\n",
        "Encoder can be defined multi-layered RNN network. For the sake of simplicity, I will implement it as an one-layer RNN network with 1 cell at each timestep.\n",
        "\n",
        "Each RNN cell receives a source word and previous hidden state as inputs.\n",
        "\n",
        "\\begin{align*}\n",
        "s_{i}=tanh(Ws_{i-1}+Ux_{i})\n",
        "\\end{align*}\n",
        "\n",
        "According to the formula, the $i^{th}$ hidden state $s_{i}$ is calculated from the $(i-1)^{th}$ hidden state and the $i^{th}$ input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTiIOnGU7vcP"
      },
      "outputs": [],
      "source": [
        "# Let's define the encoder\n",
        "class Encoder(Layer):\n",
        "    def __init__(self,\n",
        "                 tokenizer,\n",
        "                 embedding_size,\n",
        "                 hidden_units):\n",
        "        \"\"\"\n",
        "            Encoder Block in seq2seq\n",
        "\n",
        "        :param tokenizer: tokenizer of the source language\n",
        "        :param embedding_size: dimensionality of the embedding layer\n",
        "        :param hidden_units: dimensionality of the output\n",
        "        \"\"\"\n",
        "\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_units = hidden_units\n",
        "        self.tokenizer = tokenizer\n",
        "        self.embedding = Embedding(input_dim=tokenizer.vocabulary_size(),\n",
        "                                   output_dim=embedding_size)\n",
        "        self.rnn = Bidirectional(\n",
        "            merge_mode=\"sum\",\n",
        "            layer=LSTM(units=hidden_units,\n",
        "                       return_sequences=True,\n",
        "                       return_state=True))\n",
        "\n",
        "    def call(self,\n",
        "             x):\n",
        "        \"\"\"\n",
        "        :param x: [batch, time_steps]\n",
        "        :return:\n",
        "            encoder_hidden_state: [batch, hidden_state_dim]\n",
        "            state_h: [batch, hidden_state_dim]\n",
        "            state_c: [batch, hidden_state_dim]\n",
        "        \"\"\"\n",
        "        mask = tf.where(x != 0, True, False)\n",
        "        x = self.embedding(x)\n",
        "        x, forward_h, forward_c, backward_h, backward_c = self.rnn(x, mask=mask)\n",
        "\n",
        "        return x, forward_h + backward_h, forward_c + backward_c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEjvNyjvEpmx"
      },
      "source": [
        "## Attention Layer\n",
        "The Attention Mechanism used in this project is Bahdanau Attention, which is first introduced in the [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) paper by Bahdanau et al., 2015. \\\\\n",
        "To recap, at each inference step $i$, Decoder incorporates information from both the previous Decoder timestep $s_{i-1}$ and all encoder states $h=(\\{h_1, h_2, ..., h_{Tx}\\})$ to take only the most relevant words to $y_{i-1}$ through alignment model $a$.\n",
        "\n",
        "\\begin{align*}\n",
        "e_{ij} = a(s_{i-1}, h_j)=v_a^T . tanh(W_a s_{i-1} + U_a h_j)\n",
        "\\end{align*}\n",
        "\n",
        "Therefore, the context vector $c_i$ can be calculated as\n",
        "\n",
        "\\begin{align*}\n",
        "c_i = \\sum_{j=1}^{Tx} \\alpha_{ij} h_j\n",
        "\\end{align*}\n",
        "\n",
        "in which\n",
        "\\begin{align*}\n",
        "\\alpha_{ij} = \\frac{exp(e_{ij})}{\\sum_{k=1}^{Tx}exp(e_{ik})}\n",
        "\\end{align*}\n",
        "\n",
        "During the training process, we will implement Teacher Forcing by combining context vector $c_i$ with Decoder input $x_i$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUYjGqzkEpAj"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(Layer):\n",
        "    def __init__(self,\n",
        "                 hidden_units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.Va = Dense(1)\n",
        "        self.Wa = Dense(hidden_units)\n",
        "        self.Ua = Dense(hidden_units)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(BahdanauAttention, self).build(input_shape)\n",
        "\n",
        "    def call(self,\n",
        "             context, x):\n",
        "        \"\"\"\n",
        "            Calculate the context vector based on all encoder hidden states and\n",
        "            previous decoder state.\n",
        "\n",
        "        :param: context: tensor, all encoder hidden states\n",
        "        :param: state: tensor, previous state from Decoder\n",
        "        :return:\n",
        "            context_vector: tensor, the calculated context vector based on the\n",
        "            input parameters\n",
        "        \"\"\"\n",
        "        # Expand dims to ensure scores shape = [batch, Ty, Tx]\n",
        "        context = tf.expand_dims(context, axis=1)\n",
        "        x = tf.expand_dims(x, axis=2)\n",
        "\n",
        "        scores = self.Va(tf.math.tanh(self.Wa(context) + self.Ua(x)))\n",
        "        scores = tf.squeeze(scores)\n",
        "        attn_weights = tf.nn.softmax(scores, axis=-1)\n",
        "\n",
        "        # NOTE: context shape = [batch, 1, Tx, feature] so that expand\n",
        "        # dim of attention weights\n",
        "        context_vector = tf.expand_dims(attn_weights, axis=-1) * context\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=-2)\n",
        "\n",
        "        return context_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEobvMrrRRCi"
      },
      "source": [
        "## Decoder\n",
        "Encoder and Decoder share the same structure as well as hidden units but the last dense layer at each state which holds for predicting the next word using a softmax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKvgw4H_r5oG"
      },
      "outputs": [],
      "source": [
        "# Let's define the decoder\n",
        "class Decoder(Layer):\n",
        "    @classmethod\n",
        "    def add_method(cls, fun):\n",
        "        setattr(cls, fun.__name__, fun)\n",
        "        return fun\n",
        "\n",
        "    def __init__(self,\n",
        "                 tokenizer,\n",
        "                 embedding_size,\n",
        "                 hidden_units):\n",
        "        \"\"\"\n",
        "            Decoder Block in seq2seq\n",
        "\n",
        "        :param tokenizer: tokenizer of the source language\n",
        "        :param embedding_size: dimensionality of the embedding layer\n",
        "        :param hidden_units: dimensionality of the output\n",
        "        \"\"\"\n",
        "\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_units = hidden_units\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab = tokenizer.get_vocabulary()\n",
        "        self.vocab_size = tokenizer.vocabulary_size()\n",
        "        self.embedding = Embedding(input_dim=self.vocab_size,\n",
        "                                   output_dim=embedding_size)\n",
        "        self.rnn = LSTM(units=hidden_units,\n",
        "                        return_sequences=True,\n",
        "                        return_state=True)\n",
        "        self.attention = BahdanauAttention(hidden_units)\n",
        "        self.dense = Dense(15000)\n",
        "\n",
        "    def call(self,\n",
        "            context, x,\n",
        "            encoder_state,\n",
        "            return_state=False):\n",
        "        \"\"\"\n",
        "        :param trg: [batch, timesteps]\n",
        "        :param previous_state: [batch, hidden_unit_dim]\n",
        "\n",
        "        :return:\n",
        "            prediction: [vocab_size, None]\n",
        "        \"\"\"\n",
        "        mask = tf.where(x != 0, True, False)\n",
        "        x = self.embedding(x)\n",
        "        decoder_outputs, state_h, state_c = self.rnn(x, initial_state=encoder_state,\n",
        "                                                     mask=mask)\n",
        "        context_vector = self.attention(context, decoder_outputs)\n",
        "        dense_inputs = tf.concat([decoder_outputs, context_vector], axis=-1)\n",
        "        logits = self.dense(dense_inputs)\n",
        "\n",
        "        if return_state:\n",
        "            return logits, state_h, state_c\n",
        "        else:\n",
        "            return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO5K_g9PaZC9"
      },
      "source": [
        "## seq2seq\n",
        "Now we have got the Encoder and Decoder. Let's combine them into seq2seq model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "312iMCYVZZk0"
      },
      "outputs": [],
      "source": [
        "class NMT(tf.keras.Model):\n",
        "    @classmethod\n",
        "    def add_method(cls, fun):\n",
        "        setattr(cls, fun.__name__, fun)\n",
        "        return fun\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_tokenizer,\n",
        "                 output_tokenizer,\n",
        "                 embedding_size,\n",
        "                 hidden_units):\n",
        "        \"\"\"\n",
        "            Initialize an instance for Neural Machine Translation Task\n",
        "\n",
        "        :param input_tokenizer: tokenizer of the input language\n",
        "        :param output_tokenizer: tokenizer of the output language\n",
        "        :param embedding_size: dimensionality of embedding layer\n",
        "        :param hidden_units: dimensionality of the output\n",
        "        \"\"\"\n",
        "\n",
        "        super(NMT, self).__init__()\n",
        "        self.encoder = Encoder(input_tokenizer,\n",
        "                               embedding_size,\n",
        "                               hidden_units)\n",
        "        self.decoder = Decoder(output_tokenizer,\n",
        "                               embedding_size,\n",
        "                               hidden_units)\n",
        "\n",
        "    def call(self,\n",
        "             inputs):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        encoder_inputs, decoder_inputs = inputs\n",
        "        encoder_outputs, state_h, state_c = self.encoder(encoder_inputs)\n",
        "        logits = self.decoder(encoder_outputs, decoder_inputs,\n",
        "                              [state_h, state_c])\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@NMT.add_method\n",
        "def predict(self, next_inputs,\n",
        "            maxlen=40):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    def sampling(logits):\n",
        "        probs = tf.nn.softmax(logits)\n",
        "        dist = probs.numpy().squeeze()\n",
        "        idx = np.random.choice(range(self.decoder.vocab_size), p=dist)\n",
        "\n",
        "        return idx\n",
        "\n",
        "    translation = []\n",
        "    next_idx = np.asarray(self.encoder.tokenizer(next_inputs))\n",
        "\n",
        "    while next_idx.ndim != 2:\n",
        "        next_idx = tf.expand_dims(next_idx, axis=0)\n",
        "\n",
        "    encoder_outputs, state_h, state_c = self.encoder(next_idx)\n",
        "\n",
        "    next_inputs = \"[START]\"\n",
        "    next_idx = np.asarray(word_to_idx[next_inputs])\n",
        "\n",
        "    for i in range(maxlen):\n",
        "        while next_idx.ndim != 2:\n",
        "            next_idx = tf.expand_dims(next_idx, axis=0)\n",
        "\n",
        "        logits, state_h, state_c = self.decoder(encoder_outputs, next_idx,\n",
        "                                                [state_h, state_c],\n",
        "                                                return_state=True)\n",
        "        next_idx = sampling(logits)\n",
        "        next_inputs = self.decoder.vocab[next_idx]\n",
        "\n",
        "        if next_inputs == \"[END]\":\n",
        "            break\n",
        "        elif next_inputs == \"[UNK]\":\n",
        "            continue\n",
        "        else:\n",
        "            translation.append(next_inputs)\n",
        "\n",
        "    return \" \".join(translation)"
      ],
      "metadata": {
        "id": "MTqDxNDQgnRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jH-VDWZ_RpH"
      },
      "source": [
        "That's done for training. We still have things to do with inference stage. Model should be expected to translate multiple sentences at once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHMorM2BbAJn"
      },
      "outputs": [],
      "source": [
        "model = NMT(en_vec, ger_vec, embedding_size, hidden_units)\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "model.predict(\"Hello world\")"
      ],
      "metadata": {
        "id": "OSJKeSRLBnf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEvWS9X4i4B4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "342dd1a7-d0e2-4ad0-8221-ea5095fd5c56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m3470/3470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 38ms/step - accuracy: 0.5368 - loss: 3.2523 - val_accuracy: 0.6146 - val_loss: 2.5244\n",
            "Epoch 2/5\n",
            "\u001b[1m3470/3470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 39ms/step - accuracy: 0.6931 - loss: 1.8080 - val_accuracy: 0.6630 - val_loss: 1.9527\n",
            "Epoch 3/5\n",
            "\u001b[1m3470/3470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 38ms/step - accuracy: 0.7653 - loss: 1.2691 - val_accuracy: 0.7346 - val_loss: 1.3988\n",
            "Epoch 4/5\n",
            "\u001b[1m3470/3470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 38ms/step - accuracy: 0.8137 - loss: 0.9277 - val_accuracy: 0.7555 - val_loss: 1.2303\n",
            "Epoch 5/5\n",
            "\u001b[1m3470/3470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 39ms/step - accuracy: 0.8357 - loss: 0.7643 - val_accuracy: 0.7855 - val_loss: 1.0606\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(train_ds,\n",
        "                    epochs=5,\n",
        "                    validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(\"She was so angry that she could not speak.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "jYqtLgWHEdmp",
        "outputId": "212db434-5dc7-4f3a-aa70-6f38db93e154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sie war so schnell argerlich , dass sie nichts sprechen konnte .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bq14e_Bk6V8o"
      },
      "outputs": [],
      "source": [
        "model.save_weights(\"model_v4.weights.h5\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}