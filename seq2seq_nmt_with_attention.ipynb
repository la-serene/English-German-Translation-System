{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbmWAwEjlvuO"
      },
      "source": [
        "# Neural Machine Translation: a seq2seq implementation to translate English to German"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W8pS3PvNlwGC"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install \"tensorflow-text\"==2.15.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lQ9Qd9FygRzY"
      },
      "outputs": [],
      "source": [
        "# import necessary libs\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_text as tf_text\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import (Layer, Dense, LSTM, Embedding,\n",
        "                        TextVectorization, Bidirectional, Add,\n",
        "                        LayerNormalization, Activation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB9eOWcv6M9c"
      },
      "source": [
        "# Introduction\n",
        "This notebook is to implement the seq2seq model with Attention proposed by [Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ys474B6anhNe"
      },
      "outputs": [],
      "source": [
        "# Let's define some constant variables\n",
        "max_vocab_size=16000\n",
        "DROPOUT=0.3\n",
        "\n",
        "BUFFER_SIZE=1024\n",
        "BATCH_SIZE=64\n",
        "\n",
        "embedding_size=128\n",
        "hidden_units=128\n",
        "\n",
        "data_file=\"deu.txt\"\n",
        "data_dir='/content/data/'\n",
        "os.makedirs(data_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX9fhXvbgkrA"
      },
      "source": [
        "# Data Preprocessing\n",
        "We will use the [English to German](https://www.manythings.org/anki/deu-eng.zip) dataset from Manythings.org."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6hwjDWEgwYa",
        "outputId": "e860da0e-a69a-4f6c-a947-ee10b63586c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-09 17:58:21--  https://www.manythings.org/anki/deu-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10364105 (9.9M) [application/zip]\n",
            "Saving to: ‘deu-eng.zip’\n",
            "\n",
            "deu-eng.zip         100%[===================>]   9.88M  24.7MB/s    in 0.4s    \n",
            "\n",
            "2024-05-09 17:58:22 (24.7 MB/s) - ‘deu-eng.zip’ saved [10364105/10364105]\n",
            "\n",
            "Archive:  deu-eng.zip\n",
            "  inflating: data/deu.txt            \n",
            "  inflating: data/_about.txt         \n"
          ]
        }
      ],
      "source": [
        "# Download dataset\n",
        "!wget --no-check-certificate 'https://www.manythings.org/anki/deu-eng.zip' -O deu-eng.zip\n",
        "!unzip deu-eng.zip -d data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtrXPgMadK_U",
        "outputId": "2c3d3e6d-d26a-4a2c-b512-74423718ca36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go.\tGeh.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8597805 (Roujin)\n",
            "Hi.\tHallo!\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #380701 (cburgmer)\n",
            "Hi.\tGrüß Gott!\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #659813 (Esperantostern)\n",
            "Run!\tLauf!\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #941078 (Fingerhut)\n",
            "Run.\tLauf!\tCC-BY 2.0 (France) Attribution: tatoeba.org #4008918 (JSakuragi) & #941078 (Fingerhut)\n"
          ]
        }
      ],
      "source": [
        "# Take a look at the first lines\n",
        "with open(os.path.join(data_dir, data_file)) as f:\n",
        "    for n, line in enumerate(f):\n",
        "        print(line.strip())\n",
        "\n",
        "        if n == 4:\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xvp2AJCdep6E"
      },
      "source": [
        "To maintain coherence and prevent loss of word's meaning, contracted terms are expanded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9pf_Re-bOB1F"
      },
      "outputs": [],
      "source": [
        "en_contraction_map = {\n",
        "    \"let's\": \"let us\",\n",
        "    \"'d better\": \" had better\",\n",
        "    \"'s\": \" is\",\n",
        "    \"'re\": \" are\",\n",
        "    \"'m\": \" am\",\n",
        "    \"'ll\": \" will\",\n",
        "    \"'d\": \" would\",\n",
        "    \"'ve\": \" have\",\n",
        "    \"'em\": \" them\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"n't\": \" not\",\n",
        "    \"cannot\": \"can not\",\n",
        "}\n",
        "\n",
        "ger_contraction_map = {\n",
        "    \"'s\": \" ist\",\n",
        "    \"ä\": \"ae\",\n",
        "    \"ö\": \"oe\",\n",
        "    \"ü\": \"ue\",\n",
        "    \"ß\": \"ss\",\n",
        "    \"'ne \": \"eine \",\n",
        "    \"'n \": \"ein \",\n",
        "    \"am \": \"an dem \",\n",
        "    \"ans \": \"an das \",\n",
        "    \"aufs \": \"auf das \",\n",
        "    \"durchs \": \"durch das \",\n",
        "    \"fuers \": \"fuer das \",\n",
        "    \"hinterm \": \"hinter dem \",\n",
        "    \"im \": \"in dem \",\n",
        "    \"uebers \": \"ueber das \",\n",
        "    \"ums \": \"um das \",\n",
        "    \"unters \": \"unter das \",\n",
        "    \"unterm \": \"unter dem \",\n",
        "    \"vors \": \"vor das \",\n",
        "    \"vorm \": \"vor dem \",\n",
        "    \"zum \": \"zu dem \",\n",
        "    \"ins \": \"in das \",\n",
        "    \"vom \": \"von dem\" ,\n",
        "    \"beim \": \"bei dem \",\n",
        "    \"zur  \": \"zu der \",\n",
        "}\n",
        "\n",
        "def expand_contractions(text, mapping):\n",
        "    for key, value in mapping.items():\n",
        "        text = re.sub(key, value, text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJmLUQbMe9qY",
        "outputId": "dfd1879e-7ccd-4699-d997-ff8c8510289b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "He definitely did not do it. He must have been forced to commit crime. He will not do it again.\n",
            "Hinterm Haus, das an dem Fluss liegt, steht ein grosser Baum.\n"
          ]
        }
      ],
      "source": [
        "# Let's test the function\n",
        "print(expand_contractions(\"He definitely didn't do it. He must've been forced to commit crime. He won't do it again.\", en_contraction_map))\n",
        "print(expand_contractions(\"Hinterm Haus, das am Fluss liegt, steht ein großer Baum.\", ger_contraction_map))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3WaWXfEdgch"
      },
      "source": [
        "The dataset are still in unprocessed form. It is necessary to preprocess and store them in appropriate form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Tkv4Xv5jde1e"
      },
      "outputs": [],
      "source": [
        "english = []\n",
        "german = []\n",
        "\n",
        "with open(os.path.join(data_dir, data_file)) as f:\n",
        "    for line in f:\n",
        "        line = line.split(\"CC-BY\")\n",
        "\n",
        "        if len(line) > 0:\n",
        "            sample = line[0]\n",
        "            sample = sample.strip().split('\\t')\n",
        "\n",
        "            english.append(expand_contractions(sample[0].lower(), en_contraction_map))\n",
        "            german.append(expand_contractions(sample[1].lower(), ger_contraction_map))\n",
        "\n",
        "english = np.array(english)\n",
        "german = np.array(german)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd_efLVIgJ0X",
        "outputId": "ced24ae7-dd4b-4abc-8cb3-353c1934141a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this road is dangerous. --> der weg ist gefaehrlich.\n",
            "this material is not suitable for a dress. --> dieser stoff ist nicht geeignet fuer ein kleid.\n",
            "tom hates climbing ladders. --> tom steigt nicht gern auf leitern.\n",
            "why do you want to do this? --> warum willst du das tun?\n",
            "he does not get up early. --> er steht nicht frueh auf.\n"
          ]
        }
      ],
      "source": [
        "# Take a look at 5 random examples\n",
        "for i in range(5):\n",
        "    rdi = random.randint(0, len(english))\n",
        "    print(\"{:4} --> {:4}\".format(english[rdi], german[rdi]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1skosGT9QI9d"
      },
      "outputs": [],
      "source": [
        "# english = english[:400]\n",
        "# german = german[:400]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8hu1Jo3Gl7Sg"
      },
      "outputs": [],
      "source": [
        "# Create mask\n",
        "mask = np.full((len(english),), False)\n",
        "train_mask = np.copy(mask)\n",
        "train_mask[:int(len(english) * 0.8)] = True\n",
        "np.random.shuffle(train_mask)\n",
        "\n",
        "false_indices = np.where(train_mask == False)[0]\n",
        "np.random.shuffle(false_indices)\n",
        "border_idx = int(len(false_indices) / 2)\n",
        "\n",
        "val_mask = np.copy(mask)\n",
        "val_mask[false_indices[:border_idx]] = True\n",
        "\n",
        "test_mask = np.copy(mask)\n",
        "test_mask[false_indices[border_idx:]] = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "X_cRzMov9cxp"
      },
      "outputs": [],
      "source": [
        "train_raw = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((english[train_mask], german[train_mask]))\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "val_raw = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((english[val_mask], german[val_mask]))\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "test_raw = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((english[test_mask], german[test_mask]))\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WPpwFvJTn2p"
      },
      "source": [
        "It can be inferred from the plots that despite some outliers, sentence length tends to remain stable along both dataset. Therefore, it is not necessary to implement bucketing by length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkOTh8qpW3O1"
      },
      "source": [
        "# Tokenization\n",
        "Computer obviously cannot handle raw text. Instead, they need to be converted into numerical form for further calculations. Besides, while both removing punctuation and lowercasing all words are common practice in NLP tasks, it is not really the case for Neural Machine Translation. Punctuation is important to mark the start or end of a sentence. Therefore, we may well\n",
        "necessarily tokenize them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Mca-vHiH-VNv"
      },
      "outputs": [],
      "source": [
        "def text_standardize(text):\n",
        "  # Split accented characters.\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "\n",
        "  # Keep space, a to z, and select punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "\n",
        "  # Add spaces around punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "\n",
        "  # Strip whitespace.\n",
        "  text = tf.strings.strip(text)\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "10ZA4Ptzkk9l"
      },
      "outputs": [],
      "source": [
        "# Vectorizer initial\n",
        "en_vec = TextVectorization(max_tokens=max_vocab_size,\n",
        "                           standardize=text_standardize,\n",
        "                           ragged=True)\n",
        "ger_vec = TextVectorization(max_tokens=max_vocab_size,\n",
        "                            standardize=text_standardize,\n",
        "                            ragged=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "I_pKFciMqKGQ"
      },
      "outputs": [],
      "source": [
        "en_vec.adapt(train_raw.map(lambda x, y: x))\n",
        "ger_vec.adapt(train_raw.map(lambda x, y: y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "PTSyhZqlsqkW"
      },
      "outputs": [],
      "source": [
        "# Vocabulary\n",
        "en_voc = en_vec.get_vocabulary()\n",
        "ger_voc = ger_vec.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "0IgfCUpcJzKA"
      },
      "outputs": [],
      "source": [
        "# Word to Idx for prediction\n",
        "word_to_idx = {}\n",
        "\n",
        "for i in range(len(ger_voc)):\n",
        "    word_to_idx[ger_voc[i]] = i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtyXRgGj-zf9",
        "outputId": "44884c5f-391b-4e19-abb9-1a3f5b3f96e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16000\n",
            "16000\n"
          ]
        }
      ],
      "source": [
        "# Assign vocab size of each vectorizer\n",
        "input_vocab_size = len(en_vec.get_vocabulary())\n",
        "output_vocab_size = len(ger_vec.get_vocabulary())\n",
        "print(input_vocab_size)\n",
        "print(output_vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkbZvM9wl568"
      },
      "source": [
        "# Data Preparation\n",
        "Structure the dataset to use the tf.keras.models.Model's fit() method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BJMddsdymBbP"
      },
      "outputs": [],
      "source": [
        "def process_text(context, target):\n",
        "  context = en_vec(context).to_tensor()\n",
        "  target = ger_vec(target)\n",
        "  targ_in = target[:, :-1].to_tensor()\n",
        "  targ_out = target[:, 1:].to_tensor()\n",
        "  return (context, targ_in), targ_out\n",
        "\n",
        "train_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
        "val_ds = val_raw.map(process_text, tf.data.AUTOTUNE)\n",
        "test_ds = test_raw.map(process_text, tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-bVjVfLqvpg"
      },
      "source": [
        "# Model implementation\n",
        "In seq2seq, we need a RNN block, which is also known as Encoder, to encode the input sequence to a fixed-length vector, then another RNN block called Decoder to decode it. Block generally consists of LSTM cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXz0Ab-m6vBQ"
      },
      "source": [
        "## Encoder\n",
        "Encoder can be defined multi-layered RNN network. For the sake of simplicity, I will implement it as an one-layer RNN network with 1 cell at each timestep.\n",
        "\n",
        "Each RNN cell receives a source word and previous hidden state as inputs.\n",
        "\n",
        "\\begin{align*}\n",
        "s_{i}=tanh(Ws_{i-1}+Ux_{i})\n",
        "\\end{align*}\n",
        "\n",
        "According to the formula, the $i^{th}$ hidden state $s_{i}$ is calculated from the $(i-1)^{th}$ hidden state and the $i^{th}$ input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "NTiIOnGU7vcP"
      },
      "outputs": [],
      "source": [
        "# Let's define the encoder\n",
        "class Encoder(Layer):\n",
        "    def __init__(self,\n",
        "                tokenizer,\n",
        "                embedding_size,\n",
        "                hidden_units,\n",
        "                dropout=DROPOUT):\n",
        "        \"\"\"\n",
        "            Encoder Block in seq2seq\n",
        "\n",
        "        :param tokenizer: tokenizer of the source language\n",
        "        :param embedding_size: dimensionality of the embedding layer\n",
        "        :param hidden_units: dimensionality of the output\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_units = hidden_units\n",
        "        self.vocab_size = tokenizer.vocabulary_size()\n",
        "        self.embedding = Embedding(input_dim=self.vocab_size,\n",
        "                                   output_dim=embedding_size)\n",
        "        self.rnn = Bidirectional(\n",
        "            merge_mode=\"sum\",\n",
        "            layer=LSTM(units=hidden_units,\n",
        "                    dropout=dropout,\n",
        "                    return_sequences=True,\n",
        "                    return_state=True))\n",
        "\n",
        "    def call(self,\n",
        "            x,\n",
        "            training=True):\n",
        "        \"\"\"\n",
        "        :param x: [batch, time_steps]\n",
        "        :param training: is training or not\n",
        "        :return:\n",
        "            encoder_hidden_state: [batch, hidden_state_dim]\n",
        "            state_h: [batch, hidden_state_dim]\n",
        "            state_c: [batch, hidden_state_dim]\n",
        "        \"\"\"\n",
        "        mask = tf.where(x != 0, True, False)\n",
        "        x = self.embedding(x)\n",
        "        x, forward_h, forward_c, backward_h, backward_c = self.rnn(x, mask=mask,\n",
        "                                                                training=training)\n",
        "\n",
        "        return x, forward_h + backward_h, forward_c + backward_c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEjvNyjvEpmx"
      },
      "source": [
        "## Attention Layer\n",
        "The Attention Mechanism used in this project is Bahdanau Attention, which is first introduced in the [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) paper by Bahdanau et al., 2015. \\\\\n",
        "To recap, at each inference step $i$, Decoder incorporates information from both the previous Decoder timestep $s_{i-1}$ and all encoder states $h=(\\{h_1, h_2, ..., h_{Tx}\\})$ to take only the most relevant words to $y_{i-1}$ through alignment model $a$.\n",
        "\n",
        "\\begin{align*}\n",
        "e_{ij} = a(s_{i-1}, h_j)=v_a^T . tanh(W_a s_{i-1} + U_a h_j)\n",
        "\\end{align*}\n",
        "\n",
        "Therefore, the context vector $c_i$ can be calculated as\n",
        "\n",
        "\\begin{align*}\n",
        "c_i = \\sum_{j=1}^{Tx} \\alpha_{ij} h_j\n",
        "\\end{align*}\n",
        "\n",
        "in which\n",
        "\\begin{align*}\n",
        "\\alpha_{ij} = \\frac{exp(e_{ij})}{\\sum_{k=1}^{Tx}exp(e_{ik})}\n",
        "\\end{align*}\n",
        "\n",
        "During the training process, we will implement Teacher Forcing by combining context vector $c_i$ with Decoder input $x_i$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EUYjGqzkEpAj"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(Layer):\n",
        "    def __init__(self,\n",
        "                 hidden_units):\n",
        "        super().__init__()\n",
        "        self.Va = Dense(1)\n",
        "        self.Wa = Dense(hidden_units)\n",
        "        self.Ua = Dense(hidden_units)\n",
        "        self.norm = LayerNormalization()\n",
        "        self.tanh = Activation(tf.keras.activations.tanh)\n",
        "        self.add = Add()\n",
        "\n",
        "    def call(self,\n",
        "             context, x):\n",
        "        \"\"\"\n",
        "            Calculate the context vector based on all encoder hidden states and\n",
        "            previous decoder state.\n",
        "\n",
        "        :param: context: tensor, all encoder hidden states\n",
        "        :param: x: tensor, previous state from Decoder\n",
        "        :return:\n",
        "            context_vector: tensor, the calculated context vector based on the\n",
        "            input parameters\n",
        "        \"\"\"\n",
        "        # Expand dims to ensure scores shape = [batch, Ty, Tx]\n",
        "        context = tf.expand_dims(context, axis=1)\n",
        "        x = tf.expand_dims(x, axis=2)\n",
        "\n",
        "        scores = self.Va(self.tanh(self.add([self.Wa(context), self.Ua(x)])))\n",
        "        scores = tf.squeeze(scores, axis=-1)\n",
        "        attn_weights = tf.nn.softmax(scores, axis=-1)\n",
        "\n",
        "        # NOTE: context shape = [batch, 1, Tx, feature] so that expand\n",
        "        # dim of attention weights\n",
        "        context_vector = tf.expand_dims(attn_weights, axis=-1) * context\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=-2)\n",
        "        context_vector = self.norm(context_vector)\n",
        "        context_vector = self.add([context_vector, tf.squeeze(x, -2)])\n",
        "\n",
        "        return context_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEobvMrrRRCi"
      },
      "source": [
        "## Decoder\n",
        "Encoder and Decoder share the same structure as well as hidden units but the last dense layer at each state which holds for predicting the next word using a softmax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "nKvgw4H_r5oG"
      },
      "outputs": [],
      "source": [
        "# Let's define the decoder\n",
        "class Decoder(Layer):\n",
        "    def __init__(self,\n",
        "                tokenizer,\n",
        "                embedding_size,\n",
        "                hidden_units,\n",
        "                dropout=DROPOUT):\n",
        "        \"\"\"\n",
        "            Decoder Block in seq2seq\n",
        "\n",
        "        :param tokenizer: tokenizer of the source language\n",
        "        :param embedding_size: dimensionality of the embedding layer\n",
        "        :param hidden_units: dimensionality of the output\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_units = hidden_units\n",
        "        self.vocab = tokenizer.get_vocabulary()\n",
        "        self.vocab_size = tokenizer.vocabulary_size()\n",
        "        self.embedding = Embedding(input_dim=self.vocab_size,\n",
        "                                output_dim=embedding_size)\n",
        "        self.rnn = LSTM(units=hidden_units,\n",
        "                        dropout=dropout,\n",
        "                        return_sequences=True,\n",
        "                        return_state=True)\n",
        "        self.attention = BahdanauAttention(hidden_units)\n",
        "        self.dense = Dense(self.vocab_size)\n",
        "\n",
        "    def call(self,\n",
        "            context, x,\n",
        "            encoder_state,\n",
        "            training=True,\n",
        "            return_state=False):\n",
        "        \"\"\"\n",
        "        :param context: all encoder states\n",
        "        :param x: all initial decoder states\n",
        "        :param encoder_state: last state from encoder\n",
        "        :param training:\n",
        "        :param return_state:\n",
        "\n",
        "        :return:\n",
        "            logits:\n",
        "            state_h: hidden state\n",
        "            state_c: cell state\n",
        "        \"\"\"\n",
        "        mask = tf.where(x != 0, True, False)\n",
        "        x = self.embedding(x)\n",
        "        decoder_outputs, state_h, state_c = self.rnn(x, initial_state=encoder_state,\n",
        "                                                    mask=mask,\n",
        "                                                    training=training)\n",
        "        dense_inputs = self.attention(context, decoder_outputs)\n",
        "        logits = self.dense(dense_inputs)\n",
        "\n",
        "        if return_state:\n",
        "            return logits, state_h, state_c\n",
        "        else:\n",
        "            return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO5K_g9PaZC9"
      },
      "source": [
        "## seq2seq\n",
        "Now we have got the Encoder and Decoder. Let's combine them into seq2seq model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "312iMCYVZZk0"
      },
      "outputs": [],
      "source": [
        "class NMT(Model):\n",
        "    @classmethod\n",
        "    def add_method(cls, fun):\n",
        "        setattr(cls, fun.__name__, fun)\n",
        "        return fun\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_tokenizer,\n",
        "                 output_tokenizer,\n",
        "                 embedding_size,\n",
        "                 hidden_units):\n",
        "        \"\"\"\n",
        "            Initialize an instance for Neural Machine Translation Task\n",
        "\n",
        "        :param input_tokenizer: tokenizer of the input language\n",
        "        :param output_tokenizer: tokenizer of the output language\n",
        "        :param embedding_size: dimensionality of embedding layer\n",
        "        :param hidden_units: dimensionality of the output\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.input_tokenizer = input_tokenizer\n",
        "        self.output_tokenizer = output_tokenizer\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_units = hidden_units\n",
        "        self.encoder = Encoder(input_tokenizer,\n",
        "                               embedding_size,\n",
        "                               hidden_units)\n",
        "        self.decoder = Decoder(output_tokenizer,\n",
        "                               embedding_size,\n",
        "                               hidden_units)\n",
        "\n",
        "    def call(self,\n",
        "             inputs):\n",
        "        encoder_inputs, decoder_inputs = inputs\n",
        "        encoder_outputs, state_h, state_c = self.encoder(encoder_inputs)\n",
        "        logits = self.decoder(encoder_outputs, decoder_inputs,\n",
        "                              [state_h, state_c])\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"input_tokenizer\": tf.keras.utils.serialize_keras_object(self.input_tokenizer),\n",
        "            \"output_tokenizer\": tf.keras.utils.serialize_keras_object(self.output_tokenizer),\n",
        "            \"embedding_size\": self.embedding_size,\n",
        "            \"hidden_units\": self.hidden_units\n",
        "        })\n",
        "\n",
        "        return {**config}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "MTqDxNDQgnRZ"
      },
      "outputs": [],
      "source": [
        "@NMT.add_method\n",
        "def translate(self, next_inputs,\n",
        "            maxlen=40):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    def sampling(logits):\n",
        "        probs = tf.nn.softmax(logits)\n",
        "        dist = probs.numpy().squeeze()\n",
        "        idx = np.random.choice(range(self.decoder.vocab_size), p=dist)\n",
        "\n",
        "        return idx\n",
        "\n",
        "    translation = []\n",
        "    next_inputs = expand_contractions(next_inputs.lower(), en_contraction_map)\n",
        "    next_idx = np.asarray(self.encoder.tokenizer(next_inputs))\n",
        "\n",
        "    while next_idx.ndim != 2:\n",
        "        next_idx = tf.expand_dims(next_idx, axis=0)\n",
        "\n",
        "    encoder_outputs, state_h, state_c = self.encoder(next_idx, training=False)\n",
        "\n",
        "    next_inputs = \"[START]\"\n",
        "    next_idx = np.asarray(word_to_idx[next_inputs])\n",
        "\n",
        "    for i in range(maxlen):\n",
        "        while next_idx.ndim != 2:\n",
        "            next_idx = tf.expand_dims(next_idx, axis=0)\n",
        "\n",
        "        logits, state_h, state_c = self.decoder(encoder_outputs, next_idx,\n",
        "                                                [state_h, state_c],\n",
        "                                                training=False,\n",
        "                                                return_state=True)\n",
        "        next_idx = sampling(logits)\n",
        "        next_inputs = self.decoder.vocab[next_idx]\n",
        "\n",
        "        if next_inputs == \"[END]\":\n",
        "            break\n",
        "        elif next_inputs == \"[UNK]\":\n",
        "            continue\n",
        "        else:\n",
        "            translation.append(next_inputs)\n",
        "\n",
        "    return \" \".join(translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "SHMorM2BbAJn"
      },
      "outputs": [],
      "source": [
        "model = NMT(en_vec, ger_vec, embedding_size, hidden_units)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.005),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEvWS9X4i4B4",
        "outputId": "50e2bb3f-cdd0-4186-fda3-0415d863f9ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "3474/3474 [==============================] - 187s 54ms/step - loss: 0.6670 - accuracy: 0.8328 - val_loss: 0.9623 - val_accuracy: 0.7962\n",
            "Epoch 2/5\n",
            "3474/3474 [==============================] - 186s 54ms/step - loss: 0.6525 - accuracy: 0.8352 - val_loss: 0.9489 - val_accuracy: 0.7992\n",
            "Epoch 3/5\n",
            "3474/3474 [==============================] - 188s 54ms/step - loss: 0.6415 - accuracy: 0.8366 - val_loss: 0.9559 - val_accuracy: 0.7979\n",
            "Epoch 4/5\n",
            "3474/3474 [==============================] - 182s 52ms/step - loss: 0.6330 - accuracy: 0.8383 - val_loss: 0.9536 - val_accuracy: 0.7993\n",
            "Epoch 5/5\n",
            "3474/3474 [==============================] - 185s 53ms/step - loss: 0.6275 - accuracy: 0.8390 - val_loss: 0.9461 - val_accuracy: 0.8004\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(train_ds,\n",
        "                    epochs=5,\n",
        "                    validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Z1EHCekeTLEs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "386f0518-3731-4166-9a56-c8cada8b877f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "435/435 [==============================] - 16s 36ms/step - loss: 0.9508 - accuracy: 0.7982\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9508467316627502, 0.7981686592102051]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "model.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "qTxpYPNrOGAy"
      },
      "outputs": [],
      "source": [
        "model.save_weights(\"model_v8.weights.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "_igiSpE5hrP-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "b0f50b40-7bc7-4ecc-a2f9-fc3171e30245"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'komm demnaechst !'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "model.translate(\"Come on!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "byqMZGGnq2n3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}